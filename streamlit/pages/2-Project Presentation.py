import streamlit as st
from streamlit_timeline import timeline
import json
import pandas as pd
from google.cloud import bigquery
from google.oauth2 import service_account
import os 

current_dir = os.path.dirname(__file__)
def path_to_image(image_name):
    image_path = os.path.join(current_dir, "images")
    image_path=os.path.join(current_dir, image_path,image_name)
    return image_path

st.set_page_config(
    page_title="AperÃ§u du Projet",
    layout="wide"
)

# Selectbox pour choisir la langue
lang = st.sidebar.selectbox("Language / Langue", ["English", "FranÃ§ais"])

# Contenu en fonction de la langue sÃ©lectionnÃ©e
if lang == "English":
    tab1, tab2,tab3,tab4= st.tabs(["The project itself","ğŸ“‚ General structure", "ğŸ¤– Our Model", "ğŸš€ Improvement Areas"])
    with tab1:
        st.title("ğŸ§­ Project Overview")

        st.markdown("""
        Welcome to the **Reddit Fake News Detector** â€“ a data-driven system designed to classify news headlines from Reddit's r/worldnews as *real* or *fake* using natural language processing and machine learning.
        For this project, we had 2 weeks to build a **proof of concept**. The goal was to create a **pipeline** that could be used to scrape, store, and analyze news headlines from the worldNews Reddit and check their veracity.
        The project is built using **Python** and various libraries such as **Streamlit**, **Pandas**, and **Scikit-learn**. The machine learning model is based on the **BERT** architecture, which is fine-tuned for the task of fake news detection.
        This page presents the **global architecture** and the **components** of our pipeline.
        """)

        # ğŸ–¼ï¸ SchÃ©ma d'architecture
        st.header("ğŸ“Œ Global Architecture")

        st.image(path_to_image("global_pipeline.png"), caption="Overview of the project architecture")

        # ğŸ”„ Etapes
        st.header("âš™ï¸ System Components")

        st.markdown("""
        Hereâ€™s a breakdown of the pipeline:

        1. **ğŸ” Data Collection**
        - We scrape new posts from [r/worldnews](https://www.reddit.com/r/worldnews/new/) daily.
        - Metadata includes: title, source, author, timestamp, URL.

        2. **ğŸ“¥ Data Storage**
        - All the data is stored in **Google BigQuery**.
        - Tables are updated automatically every 24 hours.

        3. **ğŸ§  Machine Learning Model**
        - A **BERT-based model** is used to classify each title as *fake* or *real*.
        - Predictions are stored back in the database.

        4. **ğŸ“Š Interactive Dashboard**
        - Built with **Streamlit** to display:
            - Filterable news tables
            - Prediction results
            - Performance metrics (F1, Accuracy, etc.)

        """)


        st.success("ğŸ’¡ You can explore the live dashboard using the sidebar!")
        # ğŸ—ºï¸ Roadmap
        st.header("ğŸ“… Roadmap")
        
        with open(os.path.join(current_dir, "timeline.json"), "r") as f:
            data = json.load(f)

        # Affichage de la timeline
        timeline(data, height=700)
    with tab2:
        st.title("Apache Airflow")

        st.header("What is Airflow?")
        st.markdown("""
        Apache Airflow is an open-source workflow orchestration platform that allows you to create, schedule, and monitor workflows.
        """)

        st.header("What is a DAG?")
        st.markdown("""
        A **DAG** (Directed Acyclic Graph):

        - It describes a series of steps to follow
        - In a specific order
        - Without ever going backwards (hence "acyclic")

        In Airflow, these steps are called tasks, and the DAG is the structure that connects them.
        """)

        st.header("To summarize")
        st.markdown("""
        - Automate repetitive processes
        - Get an overview of task execution
        - Track the state of tasks
        """)

        st.write("Here is the overall goal of the project with the two existing DAGs:")
        st.image(path_to_image("airflow_copie.png"), caption="Execution DAG")
        st.write("The model_to_bq DAG")
        st.image(path_to_image("dag_airflow.png"), caption="Model DAG")
        st.image(path_to_image("ariflow.png"), caption="DAG Graph")

        st.write("This entire architecture is then deployed using IaC (Infrastructure as Code) with Terraform. Terraform is an open-source tool that allows you to create, manage, and provision cloud infrastructure using code.")
        st.image(path_to_image("infrastructure_fakenews.png"), caption="IaC")
    with tab3:
        st.write("For training our model, we used the  datasets Fake.csv and True.csv from Kaggle.")
        st.write("""
                **Dataset**: https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset
                
                - **Fake.csv**: Contains 23502 fake news headlines and their corresponding labels.
                
                - **True.csv**: Contains 21417 real news headlines and their corresponding labels.
                
                Let's have a look at the most frequent words in each dataset.
                """)
        st.image(path_to_image("wordcloudfake.png"))
        st.image(path_to_image("wordcloudtrue.png"))
        st.write("""
                The model was trained using a **BERT-based** architecture, which is a transformer model pre-trained on a large corpus of text data. 
                The model was fine-tuned on the fake news dataset to learn the specific patterns and features that distinguish fake news from real news.
                
                Finally we obtained a model with an accuracy of **97%** on the test set.
                Here is an example of how the model treats a sentence and the result associated.
                """)
        st.image(path_to_image("shap.png"))
        st.image(path_to_image("result.png"))
    with tab4:
        st.markdown("""
    ğŸ’¡ Here are some key areas for future improvements to make the project even more robust and impactful:

    ### ğŸ“š Data & Model
    - **Balance the training dataset** to avoid biased predictions.
    - **Experiment with alternative models** like DistilBERT or multilingual transformers.
    - **Data augmentation**: apply paraphrasing, synonym replacement, or back-translation techniques to enrich the dataset.

    ### ğŸ§‘â€ğŸ’» User Interface (Streamlit)
    -  **Add more interactive visualizations** such as bar charts, heatmaps, and timelines.
    -  **Allow users to submit custom news URLs** for live predictions and fact-checking.
    -  **Expand coverage to other subreddits** like r/news, r/politics, r/technology

    ### ğŸ”— Cross-Source Validation
    -  **Leverage citation networks** to validate news by checking how often an article or its claims are referenced by trusted outlets.
    -  **Build a source graph**: link articles by shared citations or referenced URLs.
    -  **Score trustworthiness** not just from language, but from cross-source validation mechanisms.

    """)
    
elif lang== "FranÃ§ais":
    tab1, tab2, tab3, tab4 = st.tabs(["Le projet", "ğŸ“‚ Structure gÃ©nÃ©rale", "ğŸ¤– Notre modÃ¨le", "ğŸš€ Domaines d'amÃ©lioration"])
    
    with tab1:
        st.title("ğŸ§­ AperÃ§u du projet")

        st.markdown("""
        Bienvenue dans le **DÃ©tecteur de Fake News sur Reddit** â€“ un systÃ¨me basÃ© sur les donnÃ©es conÃ§u pour classer les titres d'actualitÃ©s de `r/worldnews` de Reddit en *vÃ©ridique* ou *mensonger* en utilisant le traitement du langage naturel et l'apprentissage automatique.
        Pour ce projet, nous avons eu 2 semaines pour construire une **preuve de concept**. L'objectif Ã©tait de crÃ©er une **pipeline** qui pourrait Ãªtre utilisÃ©e pour scraper, stocker et analyser les titres d'actualitÃ©s de Reddit et vÃ©rifier leur vÃ©racitÃ©.
        Le projet est construit en utilisant **Python** et diverses bibliothÃ¨ques comme **Streamlit**, **Pandas** et **Scikit-learn**. Le modÃ¨le d'apprentissage automatique est basÃ© sur l'architecture **BERT**, qui est ajustÃ©e pour la tÃ¢che de dÃ©tection de fake news.
        Cette page prÃ©sente l'**architecture globale** et les **composants** de notre pipeline.
        """)

        # ğŸ–¼ï¸ SchÃ©ma d'architecture
        st.header("ğŸ“Œ Architecture globale")
        st.image(path_to_image("global_pipeline.png"), caption="Overview of the project architecture")

        # ğŸ”„ Etapes
        st.header("âš™ï¸ Composants du systÃ¨me")

        st.markdown("""
        Voici une rÃ©partition de la pipeline :

        1. **ğŸ” Collecte des donnÃ©es**
        - Nous rÃ©cupÃ©rons quotidiennement de nouveaux posts de [r/worldnews](https://www.reddit.com/r/worldnews/new/).
        - Les mÃ©tadonnÃ©es incluent : titre, source, auteur, timestamp, URL.

        2. **ğŸ“¥ Stockage des donnÃ©es**
        - Toutes les donnÃ©es sont stockÃ©es dans **Google BigQuery**.
        - Les tables sont mises Ã  jour automatiquement toutes les 24 heures.

        3. **ğŸ§  ModÃ¨le d'apprentissage automatique**
        - Un modÃ¨le **basÃ© sur BERT** est utilisÃ© pour classer chaque titre en *faux* ou *vÃ©ridique*.
        - Les prÃ©dictions sont stockÃ©es dans la base de donnÃ©es.

        4. **ğŸ“Š Tableau de bord interactif**
        - Construit avec **Streamlit** pour afficher :
            - Des tables d'articles filtrables
            - Les rÃ©sultats des prÃ©dictions
            - Les mÃ©triques de performance
        """)
        
        # ğŸ—ºï¸ Roadmap
        st.header("ğŸ“… Roadmap")
        
        with open(os.path.join(current_dir, "timeline.json"), "r") as f:
            data = json.load(f)

        # Affichage de la timeline
        timeline(data, height=700)


    with tab2:
        st.title("Apache Airflow")

        st.header("Qu'est-ce qu'Airflow ?")
        st.markdown("""
        Apache Airflow est une plateforme open-source d'orchestration de workflows qui vous permet de crÃ©er, planifier et surveiller des workflows.
        """)

        st.header("Qu'est-ce qu'un DAG ?")
        st.markdown("""
        Un **DAG** (Directed Acyclic Graph) :

        - Il dÃ©crit une sÃ©rie d'Ã©tapes Ã  suivre
        - Dans un ordre spÃ©cifique
        - Sans jamais revenir en arriÃ¨re (d'oÃ¹ "acyclique")

        Dans Airflow, ces Ã©tapes sont appelÃ©es des tÃ¢ches, et le DAG est la structure qui les relie.
        """)

        st.header("Pour rÃ©sumer")
        st.markdown("""
        - Automatiser des processus rÃ©pÃ©titifs
        - Obtenir une vue d'ensemble de l'exÃ©cution des tÃ¢ches
        - Suivre l'Ã©tat des tÃ¢ches
        """)

        st.write("Voici l'objectif global du projet avec les deux DAGs existants :")
        st.image(path_to_image("airflow_copie.png"), caption="DAG d'exÃ©cution")
        st.image(path_to_image("dag_airflow.png"), caption="DAG du modÃ¨le")
        st.image(path_to_image("ariflow.png"), caption="Graphique du DAG")

        st.write("Toute cette architecture est ensuite dÃ©ployÃ©e avec l'IaC (Infrastructure as Code) avec Terraform. Terraform est un outil open-source qui permet de crÃ©er, gÃ©rer et provisionner des infrastructures cloud en utilisant du code.")
        st.image(path_to_image("infrastructure_fakenews.png"), caption="IaC")


    with tab3:
        st.write("Pour entraÃ®ner notre modÃ¨le, nous avons utilisÃ© les ensembles de donnÃ©es Fake.csv et True.csv de Kaggle.")
        st.write("""
                    **Dataset**: https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset
                 
                    - **Fake.csv** : Contient 23 502 fake news et leurs Ã©tiquettes correspondantes.
                    - **True.csv** : Contient 21 417 vraies articles et leurs Ã©tiquettes correspondantes.
                 
                    Voyons les mots les plus frÃ©quents dans chaque jeu de donnÃ©es.
                 """)
        st.image(path_to_image("wordcloudfake.png"))
        st.image(path_to_image("wordcloudtrue.png"))
        st.write("""
                    Le modÃ¨le a Ã©tÃ© entraÃ®nÃ© avec une architecture **basÃ©e sur BERT**, un modÃ¨le de transformateur prÃ©-entraÃ®nÃ© sur un grand corpus de donnÃ©es textuelles. 
                    Le modÃ¨le a Ã©tÃ© ajustÃ© sur l'ensemble de donnÃ©es des fake news pour apprendre les motifs et les caractÃ©ristiques spÃ©cifiques qui distinguent les fake news des vraies news.
                    
                    Enfin, nous avons obtenu un modÃ¨le avec une prÃ©cision de **97%** sur l'ensemble de test.
                    Voici un exemple de la maniÃ¨re dont le modÃ¨le traite une phrase et le rÃ©sultat associÃ©.
                 """)
        st.image(path_to_image("shap.png"))
        st.image(path_to_image("result.png"))


    with tab4:
        st.markdown("""
        Comme tout projet, des amÃ©liorations sont possibles, en voici que nous avons jugÃ©s pertinentes d'Ã©voquer:

        ### ğŸ“š Meilleur modÃ¨le et jeu de donnÃ©es 
        - **Ã‰quilibrer l'ensemble de donnÃ©es d'entraÃ®nement** pour Ã©viter des prÃ©dictions biaisÃ©es.
        - **ExpÃ©rimenter avec des modÃ¨les alternatifs** comme DistilBERT ou des transformateurs multilingues.
        - **Augmentation des donnÃ©es** : appliquer des techniques de paraphrase, de remplacement de synonymes ou de back-translation pour enrichir l'ensemble de donnÃ©es.

        ### ğŸ§‘â€ğŸ’» Interface utilisateur (Streamlit)
        - **Ajouter plus de visualisations interactives** telles que des diagrammes en barres, des cartes thermiques et des chronologies.
        - **Permettre aux utilisateurs de soumettre des URLs de nouvelles personnalisÃ©es** pour des prÃ©dictions en direct et de la vÃ©rification des faits.
        - **Ã‰tendre la couverture Ã  d'autres subreddits** comme `r/news`, `r/politics`, `r/technology`.

        ### ğŸ”— Validation croisÃ©e des sources
        - **Utiliser des rÃ©seaux de citations** pour valider les articles en vÃ©rifiant combien de fois un article ou ses affirmations sont rÃ©fÃ©rencÃ©s par des mÃ©dias de confiance.
        - **Construire un graphique de sources** : lier les articles par citations partagÃ©es ou URLs rÃ©fÃ©rencÃ©es.
        - **Ã‰valuer la fiabilitÃ©** non seulement Ã  partir du langage, mais aussi Ã  partir des mÃ©canismes de validation croisÃ©e des sources.
        """)
